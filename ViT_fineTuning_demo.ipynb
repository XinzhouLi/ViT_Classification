{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT fine tuning with medical image dataset\n",
    "\n",
    "This demo is going to use the SPMS vs RRMS npy dataset to fine tuning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "from random import random\n",
    "\n",
    "import numpy as np  \n",
    "import torch   \n",
    "import torch.nn as nn  \n",
    "from transformers import ViTModel, ViTConfig  \n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import Adam  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# SPMS vs RRMS\n",
    "train_img_path = \"data/input_data/SPMS_RRMS_CONTROL/SPMSvsRRMS_img_train.npy\"\n",
    "train_lbl_path = \"data/input_data/SPMS_RRMS_CONTROL/SPMSvsRRMS_lbl_train.npy\"\n",
    "test_img_path = \"data/input_data/SPMS_RRMS_CONTROL/SPMSvsRRMS_img_test.npy\"\n",
    "test_lbl_path = \"data/input_data/SPMS_RRMS_CONTROL/SPMSvsRRMS_lbl_test.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Torch Dataset \n",
    "class MedicalImageDataset(Dataset):\n",
    "    def __init__(self, img_path, lbl_path, transform = None, distribution = None):\n",
    "        # read the npy file get all the data\n",
    "        self.images = np.load(img_path).astype(\"float16\")\n",
    "        self.labels = np.load(lbl_path).flatten()\n",
    "        self.height = self.images.shape[1]\n",
    "        self.width = self.images.shape[2]\n",
    "        \n",
    "        self.default_transforms = v2.Compose([\n",
    "            v2.Resize((224,224), antialias=True),\n",
    "            transform(),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]) if transform != None else v2.Compose([\n",
    "            v2.Resize((224,224), antialias=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        # print(self.default_transforms)\n",
    "        # print(self.images.shape, self.labels.shape)\n",
    "\n",
    "\n",
    "        unique, counts = np.unique(self.labels, return_counts=True)\n",
    "        self.num_of_patients = [9,10,10]\n",
    "        # base on the distribution filter some part of the data out\n",
    "        if distribution:\n",
    "            img0 = self.images[0:counts[0]]\n",
    "            img1 = self.images[counts[0]:counts[0]+counts[1]+1]\n",
    "            # print(img0.shape, img1.shape,counts)\n",
    "\n",
    "            img0 = self.resampled_by_patient(img0, self.num_of_patients[0], distribution[0])\n",
    "            img1 = self.resampled_by_patient(img1, self.num_of_patients[1], distribution[1])\n",
    "\n",
    "            self.images = np.concatenate((img0, img1), axis=0)\n",
    "            self.labels= np.concatenate((np.full((img0.shape[0],), 0, dtype=int),\n",
    "                                       np.full((img1.shape[0],), 1, dtype=int)), \n",
    "                                       axis=0).flatten()\n",
    "        \n",
    "\n",
    "\n",
    "    #  (has to be overwrote) define a iterator how to get the image and label by giving index\n",
    "    def __getitem__(self,i):\n",
    "        img = self.default_transforms(torch.from_numpy(self.images[i]).reshape((3, self.height, self.width)))\n",
    "        return img, self.labels[i]\n",
    "    \n",
    "    # (has to be overwrote) return the total number of samples from the dataset\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "\n",
    "    def resampled_by_patient(self, imgs, total_num, remain_num):\n",
    "        \n",
    "        num_slice_per_p = int(imgs.shape[0]/total_num)\n",
    "        picked_index = random.sample(range(total_num), total_num-remain_num)\n",
    "        picked_index.sort()\n",
    "        picked_index.reverse()\n",
    "        for i in picked_index:\n",
    "            start = i * num_slice_per_p\n",
    "            end = start + num_slice_per_p\n",
    "            imgs = np.delete(imgs, range(start,end), axis=0)\n",
    "        return imgs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
      ")\n",
      "(1916, 256, 256, 3) (1916,)\n",
      "tensor(-0.6714, dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MedicalImageDataset(train_img_path, train_lbl_path)\n",
    "# print(torch.max(train_dataset.__getitem__(0)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):  \n",
    "  \n",
    "    def __init__(self, config=ViTConfig(), num_labels=2,  \n",
    "        model_checkpoint='google/vit-base-patch16-224-in21k'):  \n",
    "        \n",
    "        super(ViT, self).__init__()  \n",
    "        \n",
    "        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)  \n",
    "        self.classifier = (  \n",
    "        nn.Linear(config.hidden_size, num_labels)  \n",
    "        )  \n",
    "    \n",
    "    def forward(self, x):  \n",
    "        \n",
    "        x = self.vit(x)['last_hidden_state']  \n",
    "        # Use the embedding of [CLS] token  \n",
    "        output = self.classifier(x[:, 0, :])  \n",
    "  \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(dataset, epochs, learning_rate, bs):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    use_mps = torch.backends.mps.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else (\"mps\" if use_mps else \"cpu\"))\n",
    "\n",
    "    # Load nodel, loss function, and optimizer\n",
    "    model = ViT().to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Load batch image\n",
    "    train_dataset = MedicalImageDataset(train_img_path, train_lbl_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "\n",
    "    # Fine tuning loop\n",
    "    for i in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0.0\n",
    "\n",
    "        for train_image, train_label in tqdm(train_dataloader):\n",
    "            output = model(train_image.to(device))\n",
    "            loss = criterion(output, train_label.to(device))\n",
    "            acc = (output.argmax(dim=1) == train_label.to(device)).sum().item()\n",
    "            total_acc_train += acc\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10 \n",
    "LEARNING_RATE = 1e-4 \n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "      Resize(size=[224, 224], interpolation=InterpolationMode.BILINEAR, antialias=True)\n",
      "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
      ")\n",
      "(1916, 256, 256, 3) (1916,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:36<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  0.082 | Accuracy:  0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:31<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  0.068 | Accuracy:  0.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:31<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  0.052 | Accuracy:  0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:30<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  0.044 | Accuracy:  0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:30<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  0.038 | Accuracy:  0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:30<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Loss:  0.036 | Accuracy:  0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:31<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Loss:  0.035 | Accuracy:  0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:32<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Loss:  0.030 | Accuracy:  0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:32<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Loss:  0.027 | Accuracy:  0.888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [01:31<00:00,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Loss:  0.037 | Accuracy:  0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trained_model = model_train(train_dataset, EPOCHS, LEARNING_RATE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):  \n",
    "  \n",
    "    use_cuda = torch.cuda.is_available()  \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")  \n",
    "    transform = v2.Compose([  \n",
    "    v2.ToTensor(),  \n",
    "    v2.Resize((224, 224)),  \n",
    "    v2.Normalize(mean=[0.5, 0.5, 0.5],  \n",
    "    std=[0.5, 0.5, 0.5])  \n",
    "    ])  \n",
    "    \n",
    "    img = transform(img)  \n",
    "    output = trained_model(img.unsqueeze(0).to(device))  \n",
    "    prediction = output.argmax(dim=1).item()  \n",
    "  \n",
    "    return id2label[prediction]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
